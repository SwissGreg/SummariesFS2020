\documentclass[8pt]{extreport}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\title{Algorithms and Probability\\ Summary}

\begin{document}
	\maketitle
	\newpage
\chapter{Graphentheorie}
\section{Basics and Definitions}
\paragraph{Graph:}
A graph is a tuple (V,E), where V is a finite non empty set of vertices and E is a set of vertice pairs indicating the edges $V \subseteq E\\ (E \subseteq \binom{V}{2} := \{(x,y)|x,y \in V, x\neq y\}) $
\paragraph{Complete (Vollständig):}
There is an edge between each pair of vertices (den. $K_{n}$)
\paragraph{Walk (Weg):} A sequence of vertices $ \langle v_{1},v_{2},\dots,v_{n} \rangle$ if $\forall$i there exists and edge from $v_{i}$ to $v_{i+1}$. The length of the walk is given by the number of steps, i.e n-1.
\paragraph{Path (Pfad):} A walk which doesnt contain any vertice more than once (den. $P_{n}$)
\paragraph{Closed Walk (Zyklus):} A walk in which $v_{1} = v_{n}$ (den. $C_{n}$)
\paragraph{Cycle (Kreis):} A closed walk with length of atleast three and the vertices $v_{1},\dots,v_{k-1}$ are pairwise distinct (in a directed graph it must have length of atleast two)
\paragraph{Loops (Schlingen):} An edge from a vertice to itself
\paragraph{Multiple edges (Mehrfachkanten):} When vertice pairs are connected by multiple edges
\paragraph{Multigraph (Multigraph):} A Graph which contains loops and multiple edges (In this lecture we assume that a graph is not a multigraph unless stated otherwise)
\paragraph{Neighbourhood (Nachbarschaft):} All outgoing and incoming edges to/from a vertice v denoted $N_{G}(v):= \{u \in V |\{v,u\} \in E\}$
\paragraph{Degree (Grad):} Indicates the size of the neighbourhood $deg_{G}(v):= |N_{G}(v)|$
\paragraph{k-regular (k-regulär):} If every vertice $v\in V$ has degree $deg(v) = k$\\
 $\circ$ \textbf{A complete graph $K_{n}$ is n-1-regular}	
\paragraph{Adjacent (Adjazent):} Two vertices u and v if there is an edge {u,v}

\paragraph{Satz 1.2} For any Graph G=(V,E) we have $\sum_{ v\in V}deg(v) = 2|E|$
\paragraph{Korollar 1.3} For any Graph G = (V,E) the number of vertices with uneven degree is even. (Direct Proof by splitting V into even and odd degree sets then use S1.2)
\paragraph{Subgraph (Teilgraph):} A Graph H = $(V_{H},E_{H})$ is a subgraph of a graph G =$(V_{G},E_{G})$ if $V_{H} \subseteq V_{G}$ and $E_{H} \subseteq E_{G}$ denoted $H \subseteq G$
\paragraph{Induced Subgraph (Induzierte Teilgraph):} If $E_{H} = E_{G} \cap \binom{V_{H}}{2}$ denoted $H = G[V_{H}]$. If there is an edge (u,v) in G and u,v are also vertices in H then there must be an edge (u,v) in H aswell
\subsection{Connectivity and Trees}
\paragraph{Connected (Zusammenhängend):} if for any Vertices s,t $\in$V there is a s-t path. A subgraph C $\subseteq$G for which this trait is maximal is called a connected component (hence for all subgraphs $H\neq C$ with $C\subseteq H \subseteq G$ is not connected
\paragraph{Cycle Free (Kreisfrei):} A Graph which doesn't contain a cycle
\paragraph{Tree (Baum):} A Graph which is Cycle free and connected
\paragraph{Leaf (Blatt):} T=(V,E) a tree and v$\in$V a vertice with deg(v) =1
\paragraph{Lemma 1.5:} T = (V,E) a tree with $|V|\geq$ 2, it follows:
\subparagraph{a):} T contains atleast 2 leafs. (Proof: If there was only one leaf \\$2|E| = \sum_{v\in V}deg(v)\geq 1 + 2(|V|-1)$ which is a contradiction to S1.6)
\subparagraph{b):} if v$\in$V is a leaf, the graph T-v is also a tree.
\paragraph{Satz 1.6} G =(V,E) a Graph with $|V|\geq 1$ vertices, the following is equivalent:
\begin{itemize}
\item G is a tree
\item G is connected and cycle free
\item G is connected and $|E| = |V| -1$
\item G is cycle free and $|E| = |V|-1$
\item for any x,y$\in$ V, G contains exactly one x-y path.
\end{itemize}
\paragraph{Forrest (Wald):} W = (V,E) a graph which is cycle free, every component of a forrest is a tree
\paragraph{Lemma 1.7} A forrest G = (V,E) contains $|V| -|E|$ connected components (Proof by induction)
\paragraph{Directed Graph (Gerichteter Graph):} A graph where the edges are represented by ordered pairs, i.e The directed graph D is given by the tuple (V,A) where V is the set of vertices and $A\subseteq V x V$ a set of directed edges. Compared to an undirected graph, between two vertices there can be two edges (x,y) and (y,x)
\paragraph{Out-Degree(Aus-Grad):} $deg^{+}(v):=|\{(x,y)\in A | x = v\}|$
\paragraph{In-Degree(In-Grad):} $deg^{-}(v):=|\{(x,y)\in A | y = v\}|$
\paragraph{Satz 1.8} For any directed graph D = (V,A) the following is true.\\ $\sum_{v\in V}deg^{-}(v) = \sum_{v\in V}deg^{+}(v) = |A|$.
\paragraph{Acyclic (Azyklisch):} A directed graph which deosn't contain a cycle (DAG). DAG's have a topological ordering.
\paragraph{Satz 1.9} For any DAG D=(V,A) we can find a topological ordering in $ \mathcal{O}(|V| + |A|)$
\paragraph{Strongly Connected (start zusammenhängend):} For a DAG D=(V,A), if for every pair of vertices u,v$\in$V a directed u-v-Path exists
\paragraph{Weakly Connected (schwach zusammenhängend):} When the underlying graph(i.e ignoring the direction of the edges) is connected
\subsection{Datastructures} The two main ways of storing graphs, is with Adjacency matrices and Adjacency lists.
\section{Trees}
\section{Paths}
\paragraph{Shortest Paths}
Given: A connected Graph G = (V,E), two vertices s,t$\in$V and a costfunction $c:E\rightarrow \mathbb{R}$\\
Goal: Find an s-t-path P in G with $\sum_{e\in P}c(e) = min$\\
Algorithms which can solve shortest path problems: 
\begin{enumerate}
\item Dijkstras Algorithm
\item Floyd-Warshall
\item Bellman-Ford
\item Johnson's Algorithm
\end{enumerate}
\section{Connection}
\paragraph{Definition 1.23} A Graph G = (V,E) is \textbf{k-connected (k-zusammenhängend)} if $|V| \geq k+1$ and for all subsets$X\subseteq V$ with $|X|<k$ the following is true: The Graph $G[V\backslash X]$ is connected. (Hence you would need to remove atleast k vertices to destroy the connectivity of the graph, the only exception is a complete graph which is by definition k-1 connected)
\paragraph{Definition 1.24} A Graph G = (V,E) is \textbf{k-edge-connected (k-kanten-zusammenhängend)}, if for all subsets $X\subseteq E$ with $|X|<k$ the following is true: $(V,E\backslash X)$ is connected. (Hence atleast k edges must be removed to detroy the connectivity of the Graph)
\paragraph{Satz 1.25 Menger} G = (V,E) the following applies:
\begin{enumerate}
\item G is k-connected iff for all pairs of vertices u,v$\in$V, u$\neq$v, atleast k internal-vertice disjoint u-v paths exist
\item G is k-edge-connected iff for all pairs of vertices u,v$\in$V, u$\neq$v, atleast k edge-disjoint u-v-paths exist
\end{enumerate}
\subsection{Articulation vertice (Artikulationsknoten)}
\paragraph{Articulation vertice (Artikulationsknoten):} If a graph is connected but not 2-connected, then there exists a vertice v with the attribute that $G[V\backslash \{v\}]$ is not connected. Articulation vertices kann be detected using a modified DFS.
\paragraph{Forward Edge (Vorwärtskante):} An edge starting from a vertice with a lower dfs number than the destination vertice
\paragraph{Backwards Edge(Rückwärtskante):} An edge starting from a vertice with a higher dfs number than the destination vertice
\paragraph{} we assign $\in$V a number low[v]:= the smallest dfs-Number, that can be reached from the vertice v using any number of forward edges and atmost one backward edge. It follows for all v$\in$V: $low[v]\leq dfs[v]$ 
\paragraph{} v is an Articulation vertice $\Leftrightarrow$ v=s and s has degree of atleast 2 or v$\neq$s and there exists a w $\in$ V with \{v,w\} $\in$ E(T) and low[w] $\geq$ dfs[v] 
\paragraph{TODO:} Implement DFS-Visit which finds the articulation vertices for a given graph
\paragraph{Satz 1.27} For a connected graph G = (V,E), implemented with an adjacency list Articulation vertices can be found in $ \mathcal{O}(|E|)$
\subsection{Bridges (Brücken)}
\paragraph{Bridge (Brücke):} An edge e$\in$E such that $(V,E\backslash \{e\})$ is not connected
\paragraph {}From the definition of the bridge it follows that a spanning tree must contains all bridges of a graph and that the vertices at the end of the bridge are either Articulation vertices or vertices with degree 1.
\paragraph{} An edge (v,w) of the depth-first-search tree is a bridge iff low[w] > dfs[v]
\paragraph{Satz 1.28} For a connected graph G=(V,E) implemented with an adjacency list, articulation vertices and bridges can be found in $\mathcal{O}(|E|)$
\section{Cycles}
\subsection{Eulerwalk (Eulertour)}
\paragraph{Definition 1.29} A Eulerwalk in a graph G=(V,E) is a cycle which contains each edge exactly once. If G contains a Eulerwalk then deg(v) of all v$\in$V is even. (Proof by contradiction assume G has vertices of even degrees pick a starting node v and an arbitrary node u and show that the path cannot end in u arguing with the parity of the degree)
\paragraph{} In a connected graph eulerian graph a Eulerwalk can be found in $\mathcal{O}(|E|)$
\paragraph{TODO:} Implement an Algorithm which can find a Eulerwalk in $ \mathcal{O}(|E|)$
\subsection{Hamilton Cycles (Hamiltonkreise)}
\paragraph{Defintion 1.31} A Hamilton Cycle in a graph G = (V,E) is a cycle in which all vertices of V are visited exactly once. If a graph contains a Hamilton Cycle it is called hamiltonian (hamiltonisch). Wether or not a graph contains a hamilton cycle is NP-complete.
\paragraph{Satz 1.33} The Algorithm HAMILTONKREIS to find a Hamilton cycle of a given graph G needs $\mathcal{O}(n*2^{n})$ memory and  has a runtime of $\mathcal{O}(n^{2}*2^{n})$, where $n=|V|$
\paragraph{TODO:} Implement the Algorithm HAMILTONKREIS
\subsection{Special Cases}
\paragraph{Lattice (Gittergraph)} An m x n lattice is hamiltonian if m or n is even (Proof using parity argument)
\paragraph{Lemma 1.35} $G =(A \uplus B, E)$ a bipartite Graph with $|A| \neq |B|$, then G cannot contain a Hamilton cycle
\paragraph{Hypercube (Hyperwürfel):} The set of edges of a Hypercube $H_{d}$ is $\{0,1\}^{d}$ hence the set of all 0-1 sequences of length d. Two vertices are connecten if their sequences differ at exactly one spot. A d-dimensional Hypercube contains a Hamilton cycle for all d$\geq$2 (Proof by induction)
\paragraph{Satz 1.37(Dirac)} If G = (V,E) is a graph with $|V| \geq 3$ vertices and each vertice has atleast $|V|/2$ neighbours,then G is hamiltonian.
\subsection{The Travelling Salesman Problem}
\paragraph{Given:} A complete Graph $K_{n}$ and a function $\mathit{l} : \binom{[n]}{2} \rightarrow \mathbb{N}$ which gives each edge a length
\paragraph{Goal:} Find a Hamilton cycle C in $K_{n}$ with $\sum_{e\in C'} \mathit{l}(e) = min\{\sum_{e\in C'} \mathit{l}(e) | \text{C' is a Hamilton cycle in } K_{n}\}$
\paragraph{} For a graph G = (V,E) with V = [n] vertices we define a weight function $\mathit{l}$ as $\mathit{l}(\{u,v\}) = \begin{cases}  0\text{, if} \{u,v\}\in E\\
1, \text{else} 
\end{cases}
$\\ hence the length of a minimal Hamilton cycle in $K_{n}$ when using $\mathit{l}$ is 0 iff G contains a Hamiltoncycle, hence this allows us evaluate the efficiency of a solution. We define the optimum solution as $opt(K_{n},\mathit{l}) := min\{\sum_{e\in C'} \mathit{l}(e) | \text{C' is a Hamilton cycle in } K_{n}\}$. An algorithm which always finds a Hamilton cycle with $\sum_{e\in C} \mathit{l}(e) \leq \alpha * opt(K_{n},l)$ is known as an $\alpha$-Approximation algorithm.
\paragraph{Satz 1.39} If there exists an $\alpha$-Approximationalgorithm for an $\alpha > 1 $ for the Traveling Salesman Problem with a runtime of $\mathcal{O}(f(n))$, then an algorithm exits for all graphs with n vertices which decides wether it is hamiltonian or not in the same time complexity
\paragraph{Metric Travelling Salesman Problem (Metrisches TSP)}
\paragraph{Given:} A complete Graph $K_{n}$ and a function $\mathit{l} : \binom{[n]}{2} \rightarrow \mathbb{N}$ with the condition $\mathit{l}(\{x,z\}) \leq \mathit{l}(\{x,y\}) + \mathit{l}(\{y,z\})$ for all x,y,z $\in$ [n]
\paragraph{Goal:} Find a Hamilton cycle C in $K_{n}$ with  $\sum_{e\in C'} \mathit{l}(e) = min\{\sum_{e\in C'} \mathit{l}(e) | \text{C' is a Hamilton cycle in } K_{n}\}$ The condition is called the triangle inequality (Dreiecksungleichung) and states that the direct connection between two vertices x and z cannot be longer than the detour over the vertice y.
\paragraph{Satz 1.40} The Metric Travelling Salesman Problem has a 2-Approximationsalgorithm with a runtime of $\mathcal{O}(n^2)$
\section{Matchings}
\paragraph{Matching} A set of edges $M\subseteq E$ is called a Matching of a graph G = (V,E), if no vertice of the graph is incident to more than one edge from M i.e $e\cap f = \emptyset$ for all e,f$\in$ M with $e\neq f$. A vertice v is "covered" (überdeckt) if there is an edge $e\in M$ which contains v.
\paragraph{Perfect Matching} If each vertice is covered by exactly one edge of the Matching M i.e $|M| = |V|/2$. Not all graphs contain a perfect matching for example a star graph.
\paragraph{Maximal matching (Inklusionsmaximal):} G = (V,E), for a Matching M if $M\cup \{e\}$ is not a Matching for all edges $e\in E\backslash M$.
\paragraph{Maximum matching (Kardinalitätsmaximal):} G = (V,E) for a Matching M if $|M|\geq |M'|$ for all Matchings M' in G
\paragraph{Example:} A path consisting of three edges. Creating a Matching using the middle edge would creat a Maximal matching but not a Maximum matching. A Maximum and a Maximal Matching can be created by taking the two outer edges
\subsection{Algorithms}
\paragraph{GREEDY MATCHING} This algorithm picks random edges from E and adds it to the matching at the same time it deletes all incident edges from E. The algorithm stops when $E=\emptyset$. This algorithm can find a Maximal matching in time $\mathcal{O}(|E|)$ for which the following applies: $|M_{Greedy}| \geq \frac{1}{2} |M_{max}|$ where $M_{max}$ is a Maximum matching
\paragraph{Union of two Matchings:} Let $M_{1}$ and $M_{2}$ be arbitrary Matchings. $G_{M} = (V;M_{1} \cup M_{2})$. Every vertice in $G_{M}$ has degree of atmost 2, hence all components of the graph are paths and/or cycles (cycles having even length). If we assume $|M_{1}| < |M_{2}|$, then every cycle/path of even length will have the same amount of edges from $M_{1}$ as $M_{2}$. From our assumption we know that there must be a path P which contains more edges from $M_{2}$ than $M_{1}$ the two outter edges belonging to $M_{2}$. Hence we can create a new Matching $M_{1}'$ using P which will contain one more edge. We achieve this by switching the edges in P i.e $M_{1}':= (M_{1} \cup (P \cap M_{2})) \backslash (P \cap M_{1})$. We say P is a $M_{1}$-augmented path.
\paragraph{M-augmented Path (augmentierender Pfad):} Let M be an arbitrary Matching, an M augmented path is a path where the last two edges of P are not covered by M and P consists of edges alternating between edges belonging to M and not belonging to M
\paragraph{AUGMENTED MATCHING} This algorithm finds a Maximum matching. We start by finding a Matching which consists of only one arbitrary edge. As long as the matching is not a maximum matching we repeat the following: We take an augmented path and we increase the size of the Matching. We know that after $|V|/2-1$ times the matching will be maximum because a matching can't have more than $|V|/2$ edges. We can easily find augmented paths in a bipartite Graph using a modified BFS. The total runtime of our algorithm is $\mathcal{O}(|V|\cdot|E|)$
\paragraph{Satz 1.45} If n is even and $\mathit{l}: \binom{[n]}{2} \rightarrow \mathbb{N}$ a weight function of the complete graph $K_{n}$ then we can find a minimal perfect Matching (i.e a matching where the sum of edge weights are minimal) in $\mathcal{O}(n^3)$
\paragraph{Satz 1.46} From S1.45 if follows that for the Metric Travelling Salesman Problem there is a 3/2-Approximationsalgorithm with a runtime of $\mathcal{O}(n^3)$
\subsection{Der Satz von Hall}
\paragraph{Bipartite} A graph G=(V,E) is bipartite if we can partition the set of vertices V into two sets A and B such that all edges in E contain a vertice from A and a vertice from B (denoted: $G=(A \uplus B, E)$)
\paragraph{Satz von Hall/Heiratssatz:} For a bipartite graph $G=(A \uplus B, E)$ there is a Matching M of cardinality $|M| = |A|$ iff $|N(X)| \geq |X|$ for all X$\subseteq$ A. From the satz von Hall it follows that a k-regular graph always has a perfect matching.
\paragraph{Satz 1.48} Let $G=(A\uplus B,E)$ be a k-regular bipartite graph. There exists an $M_{1},\dots ,M_{k}$ such that $E = M_{1} \uplus \dots  \uplus M_{k}$ and all $M_{i}, 1\leq i\leq k$ are perfect matchings in G. The perfect matching can be found in O(|E|).
\paragraph{Satz 1.49} Let G=(V,E) a $2^k$-regular bipartite graph. We can find a perfect matching in $\mathcal{O}(|E|)$
\section{Colouring (Färbungen)}
\paragraph{Vertex colouring (Knoten Färbung):} The vertex colouring of a graph G = (V,E) with k colours is a mapping $c:V\rightarrow [k]$ such that the following holds: $c(u) \neq c(v)$ for all edges {u,v}$\in$ E
\paragraph{Chromatic number (chromatische Zahl):} dentoted $\mathit{x}(G)$ is the minimal number of colours needed to color the vertices of G. A complete graph has the chromatic number n. Cycles of even length have chromatic number 2, uneven length have a chromatic number 3. Trees with atleast two vertices have a chromatic number 2. Graphs with chromatic number k are also called k-partite. To decide wether or not a graph G is bipartit can be done in $O(|E|)$ with a DFS of BFS.
\paragraph{Satz 1.53} A graph G=(V,E) is bipartite iff it does not contain a cycle of odd length as a subgraph
\paragraph{Satz 1.54 (Vierfarbensatz):} Any map can be coloured using 4 colours.
\paragraph{GREEDY FARBUNG} This algorithm calculates the colouring of a graph by picking vertices at random and giving it the lowest colour not used by its neighbours. There exists a order of vertices for which the GREEDY algorithm needs $\mathit{x}(G)$ colors.
\paragraph{Satz 1.55} Let G be a connected graph. For the number C(G) of colours needed by GREEDY FARBUNG to color the graph G the following applies: $\mathit{x}(G) \leq C(G) \leq \Delta(G) + 1$ ($\Delta (G) := max_{v\in V}deg(v)$, hence the max degree of a vertice in G). If the graph is saved in an adjacency list then we can find the colouring in $\mathcal{O}(|E|)$
\paragraph{Satz 1.59 (Satz von Brooks)} Let G=(V,E) be a connected graph which is not complete nor a cycle with odd degree i.e $G\neq K_{n}$ and $G\neq C_{2n+1}$ then the following holds $\mathit{x}(G) \leq \Delta(G)$ and there exists and Algorithm which can colour the graph in $\mathcal{O}(|E|)$ with $\Delta (G)$ colours.
\paragraph{Satz 1.60} Let G=(V,E) be a graph and $k\in \mathbb{N}$ a natural number such that every induced subgraph of G has a vertice with degree atmost k. It follows that $\mathit{x}(G)\leq k+1$ and we can find a (k+1)-colouring in $\mathcal{O}(|E|)$
\paragraph{Satz 1.61 (Mycielski-Konstruktion):} For all k $\geq$2 there is a triangle free graph $G_{k}$ with $x(G_{k}) \geq k$ (Proof by induction)
\paragraph{Satz 1.62} Every 3-colourable graph G=(V,E) can be coloured in $\mathcal{O}(|E|)$ with $\mathcal{O}(\sqrt{|V|})$
colours. Given a graph G=(V,E), is $\mathit{x}(G) \leq 3$ is NP-Complete.
\chapter{Probability Theory and Randomised Algorithms}
\section{Definitions and Notations}
\paragraph{Definition 2.1} A $\mathbf{discrete \ Probabilityspace (diskreter \ Wahrscheinlichkeitsraum)}$ is defined by a Set of $\mathbf{outcomes\ (Ergebnismenge)}$ denoted $\Omega=\{\omega_{1},\omega_{2},\dots \}$ of $\mathbf{elementary\ outcomes (Elementarereignissen)}$. Each elementary outcome $\omega_{i}$ is assigned a $\mathbf{(elementary) probability (Elementar- Wahrscheinlichkeit)}$ denoted $Pr[\omega_{i}]$ where $0\leq Pr[\omega_{i}] \leq 1$ and $\sum_{\omega \in \Omega}Pr[\omega] = 1 $. A set $E\subseteq \Omega$ is called an $\mathbf{outcome (Ereignis)}$ The probability of Pr[E] of an outcome is defined by: $Pr[E] := \sum_{\omega \in E} Pr[\omega]$\\ If E is an outcome, we define $\bar{E}:= \Omega \backslash E$  the  $\mathbf{compliment\ outcome (Komplementarereignis)}$ 
\paragraph{\underline{Finite probabilityspace (endlicher Wahrscheinlichkeitsraum):}} A Probability space $\Omega = \{\omega_{1},\dots,\omega_{n}\}$ (Assumption for infinite probability spaces $\Omega =\mathbb{N}_0$)
\paragraph{\underline{Lemma 2.2}} For outcomes A,B the following applies:
\begin{enumerate}
\item{$Pr[\emptyset] = 0, Pr[\Omega] = 1$}
\item{$0 \leq Pr[A] \leq 1$}
\item{$Pr[\hat{A}] = 1 - Pr[A]$}
\item{if $A\subseteq B, Pr[A] \leq Pr[B]$}
\item (Additionssatz) Wenn die Ereignisse $A_{1},\dots, A_{n}$ paarweise disjunkt sind (also wenn für alle Paare $i\neq j$ gilt, dass $A_{i} \cap A_{j} = \emptyset$) so folgt:
\begin{center}
$Pr\bigg[\bigcup\limits_{i=1}^{n} A_{i} \bigg] = \displaystyle\sum_{i=1}^{n}Pr[A_{i}]$
\end{center}
\end{enumerate}
\paragraph{\underline{Union Bound (Boolesche Ungleichung):}} Für beliebige Ereignisse $A_{1},\dots,A_{n}$ gilt:
\begin{center}
$Pr\bigg[\bigcup\limits_{i=1}^{n} A_{i} \bigg] \leq \displaystyle\sum_{i=1}^{n}Pr[A_{i}]$
\end{center}
\paragraph{\underline{Siebformel, Prinzip Inklusion/Exklusion:}} Für Ereignisse $A_{1},\dots,A_{n} (n\geq2)$ gilt:
\begin{align*}
	Pr\bigg[\bigcup\limits_{i=1}^{n} A_{i} \bigg] =  & \displaystyle\sum_{i=1}^{n}Pr[A_{i}] -\displaystyle\sum_{1\leq i_{1} \leq i_{2} \leq n}Pr[A_{i_{1}} \cap A_{i_{2}}] +- \dots \\
	 &+ (-1)^{l+1}\displaystyle\sum_{1\leq i_{1} < \dots < i_{l} \leq n}Pr[A_{i_{1}} \cap \dots \cap A_{i_{l}}] +- \dots \\ &+  (-1)^{n+1} \cdot Pr[A_1 \cap \dots \cap A_n]
 \end{align*}
\paragraph{\underline{Laplace Raum:}} endlicher Wahrscheinlichkeitsraum, in dem alle Elementarereignisse gleich wahrscheinlich sind.
In einem Laplace-Raum gilt für jedes Ereignis E: 
\begin{center}
$Pr[E] = \frac{|E|}{|\Omega|}$
\end{center}
\paragraph{\underline{$\#$ Möglichkeiten k Elemente aus einer n-elementigen Menge zu ziehen}}:
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{AWpic1.png}
	\caption{}
	\label{ziehungsmoglichkeiten}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{AWpic2.png}
	\caption{}
	\label{ziehungsmoglichkeiten}
	\end{subfigure}
\end{figure}
($n^{\underline{k}} := n(n-1)(n-2) \dots (n-k-1) = \frac{n!}{(n-k)!}$)
\paragraph{\underline{Bedingte Wahrscheinlichkeit:}} A und B seien Ereignisse mit $Pr[B] > 0$. Die bedingte Wahrscheinlichkeit $Pr[A|B]$ (Die W'keit, dass Ereignis A eintrifft, wenn wir schon wissen dass Ereignis B eingetreten ist) von A gegeben B is t definiert durch:
\begin{center}
$Pr[A|B] := \frac{Pr[A\cap B]}{Pr[B]}$
\end{center}
\paragraph{\underline{Satz 2.12 Satz der totalen W'keit:}} Die Ereignisse $A_1,\dots,A_n$ seien paarweise disjunkt und es gelte $B \subseteq A_1 \cup \dots \cup A_n$ dann folgt:
\begin{center}
$Pr[B] =\displaystyle\sum_{i=1}^{n}Pr[A_{i} \cap B] = \displaystyle\sum_{i=1}^{n} Pr[B|A_i] \cdot Pr[A_i]$
\end{center}
\paragraph{\underline{Satz 2.10 Multiplikationsatz:}} Seien die Ereignisse $A_1,\dots, A_n$ gegeben. Falls $Pr[A_1 \cap \dots \cap A_n] > 0$ ist, gilt:
\begin{center}
$Pr[A_1 \cap \dots \cap A_n] = Pr[A_1] \cdot Pr[A_2|A_1] \cdot Pr[A_3|A_1 \cap A_2] \dots Pr[A_n|A_1\cap \dots \cap A_{n-1}]$
\end{center}
\paragraph{\underline{Satz von Bayes:}} Die Ereigniss $A_{1},\dots,A_{n}$ seien paarweise disjunkt. Ferner $B\subseteq A_1 \cup \dots \cup A_n$ ein Ereignis mit $Pr[B] > 0$ Dann gilt für ein beliebiges $i=1,\dots, n$:
\begin{center}
$Pr[A_i|B] =\frac{Pr[A_i \cap B]}{Pr[B]} = \frac{Pr[B|A_i]\cdot Pr[A_i]}{\displaystyle\sum_{j=1}^{n} Pr[B|A_j] \cdot Pr[A_j]}$
\end{center}
\paragraph{\underline{Unabhängigkeit Zwei Ereignisse:}} Die Ereignisse A und B heissen unabhängig, wenn gilt:
\begin{center}
$Pr[A\cap B] = Pr[A] \cdot Pr[B]$
\end{center}
\paragraph{\underline{Unabhängigkeit:}} Die Ereignisse $A_1, \dots, A_n$ heissen unabhängig, wenn für alle Teilmengen $I\subseteq \{1, \dots, n\}$ mit $I = \{i_1, \dots ,i_k\}$ gilt, dass
\begin{center}
$Pr[{A_1}_1, \cap \dots \cap {A_i}_k] = Pr[{A_i}_1] \dots Pr[{A_i}_k] \qquad (2.2)$
\end{center}
Eine unendliche Familie von Ereignissen $A_i$ mit $ i \in \mathbb{N}$ heisst unabhängig, wenn (2.2) für jede endliche Teilmenge $I\subseteq \mathbb{N}$ erfüllt ist. Dies ist offensichtlich erfüllt, wenn die Ereignisse physikalisch unabhängig sind (e.g wenn jedes $A_i$ einem unabhängigem Münzwurf entspricht) aber ist nicht unbedingt erforderlich.\\
Die Ereignisse $A_1, \dots , A_n$ sind genau dann unabhängig wenn für alle $(s_1,\dots , s_n) \in \{0,1\}^n$ gilt dass
\begin{center}
$ Pr[{A_1}^{s_1} \cap \dots \cap {A_n}^{s_n}] = Pr[{A_1}^{s_1}] \dots Pr[{A_n}^{s_n}]$
\end{center}
wobei ${A_i}^0 = \overline{A_i}$ und ${A_i}^1 = {A_i}$
\newline
Seien A, B und C unabhängige Ereignisee. Dann sind auch $A \cap B$ und C bzw. $A \cup B$ und C unabhängig.
\section{Zufallsvariablen} Eine funktion welche jede element unser Wahrscheinlichkeitsraum ein Reellezahl zuordnet.
\begin{center}
$ X : \Omega \rightarrow \mathbb{R} $
\end{center}
\underline{Beispiel:}\\
"X $\leq$ 5 steht für das Ereignis, dass die Zufallsvariable einen Wert kleiner gleich 5 annimmt also:\\
$\Rightarrow X \leq 5 \hat{=} \{ \omega \in \Omega : X(\omega) \leq 5 \}$
 \paragraph{\underline{Dichtefunktion:}}
\begin{center}
$f_X:\mathbb{R} \rightarrow [0,1], \quad x \mapsto Pr[X = x]$
\end{center}
\paragraph{\underline{Verteilungsfunktion:}}
\begin{center}
$F_X : \mathbb{R} \rightarrow [0,1], \quad x \mapsto Pr[X \leq x] = \displaystyle\sum_{x' \in W_{X}: x' \leq x} Pr[X = x']$
\end{center}
\underline{Beispiele:}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{AWpic3.png}
	\caption{}
	\label{ziehungsmoglichkeiten}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{AWpic4.png}
	\caption{}
	\label{ziehungsmoglichkeiten}
	\end{subfigure}

\end{figure}
\newpage

\paragraph{\underline{Bernoulli-Verteilung:}}
\begin{center}
$X \sim$ Bernoulli(p)\\
\[ f_X(x) =
  \begin{cases}
    p       & \quad \text{für } x = 1,\\
    1-p  & \quad \text{für } x=0\\
    0 & \quad \text{sonst}
  \end{cases}
\]
$\mathbb{E}[X] = p$ 
\end{center}
\paragraph{\underline{Binomial Verteilung:}}
\begin{center}
$ X \sim$ Bin(n,p)\\
\[ f_X(x) =
\begin{cases}
\binom{n}{x}p^x(1-p)^{n-x}, \quad x \in \{0,1,\dots, n\}\\
0, \quad sonst.
\end{cases}
\]
$\mathbb{E}[X] = np \qquad Var[X] = np(1-p)$ (gleichung für Varianz gilt nur wenn die $X_i$'s unabhängig sind)
\end{center}
$Bin(n,\frac{\lambda}{n})$ konvergiert für $n\to \infty$ gegen Po($\lambda$)
Beispiel: Werfen einer Münze n mal, X = Anzahl Kopf
\paragraph{\underline{Negative Binomialverteilung:}}
\begin{center}
$X \sim$ NegativeBinomial(n)
\[ f_X(k) =
\begin{cases}
\binom{k-1}{n-1}(1-p)^{k-n}p^n, \quad \text{ für } k=1,2,\dots\\
0, \qquad sonst
\end{cases}
\]
$\mathbb{E}[X] = \frac{n}{p}$
\end{center} 
Beispiel: Warten auf den n-ten Erfolg
\paragraph{\underline{Geometrische Verteilung:}}
\begin{center}
$ X \sim$ Geo(p)
\[ f_X(i) =
	\begin{cases}
	p(1-p)^{i-1} \quad \text{ für } i \in \mathbb{N}\\
	0 \qquad sonst.
	\end{cases}
\]
$F_X(n) = 1-(1-p)^n \quad$ für alle n=1,2,...\\
$\mathbb{E}[X] = \frac{1}{p} \qquad Var[x] = \frac{1-p}{p^2}$
\end{center}
Beispiel: Wiederholtes werfen einer Münze, X = $\#$ Würfe bis zum ersten Mal Kopf\\ \\
\underline{Gedächtnislosigkeit:} Ist X $\sim$ Geo(p), so gilt fpr alle s,t $\in \mathbb{N}$:
\begin{center}
$Pr[X \geq s + t | X > s] = Pr[X \geq t]$
\end{center}
Beispiel: Wahrscheinlichkeit im ersten Wurf Kopf zu bekommen ist identisch zur Wahrscheinlichkeit nach 1000 Fehlversuchen im 1001ten Wurf Kopf zu bekommen.
\paragraph{\underline{Poisson-Verteilung:}}
\begin{center}
$X \sim Po(\lambda)$
\[ f_X(i)
\begin{cases}
\frac{e^{-\lambda}\lambda^i}{i!} \quad \text{ für } i \in \mathbb{N}_0\\
0 \qquad sonst
\end{cases}
\]
$\mathbb{E}[X] = Var[X] =  \lambda$
\end{center}
Beispiel: Modellierung seltener Ereignisse e.g X:= $\#$ Herzinfarkte in der Schweiz in der nächsten Stunde
\paragraph{\underline{Erwartungswert:}} Den Zufallsvariablen X definieren wir den Erwartungswert $\mathbb{E}[X]$ durch:
\begin{center}
$\mathbb{E}[X]:= \displaystyle\sum_{x \in W_X} x \cdot Pr[X = x]$
\end{center}
sofern die Summe konvergiert. Ansonsten sagen wir, dass der Erwartungswert undefiniert ist.\\
Ist X eine Zufallsvariable, so gilt:
\begin{center}
$\mathbb{E}[X] = \displaystyle\sum_{\omega \in \Omega} X(\omega) \cdot Pr[\omega]$
\end{center}
Sei X eine Zufallsvariable mit $W_x \subseteq \mathbb{N}_0$. Dann gilt
\begin{center}
$\mathbb{E}[X] = \displaystyle\sum_{i=1}^{\infty}Pr[X \geq i]$
\end{center}
\underline{Beobachtung:} Für ein Ereignis $A \subseteq \Omega$ ist die zugehöruge Indikatorvariable $X_A$ definiert durch:
\begin{center}
 \[ X_A(\omega) =
  \begin{cases}
    1       & \quad \text{falls } \omega \in A\\
    0  & \quad \text{sonst. }
  \end{cases}
\] 
\end{center}
Für den Erwartungswert von $X_A$ gilt: $\mathbb{E}[X_A] = Pr[A]$\\
\begin{itemize}
\item \underline{Schnitt:} $A \cap B \qquad X_{A\cap B} = X_A \cdot X_B$
\item \underline{Komplement:} $\overline{A}:= \Omega \backslash A \qquad X_{\overline{A}} = 1 - X_A$
\item \underline{Vereinigung:} $A \cup B \qquad X_{A\cup B}$
\end{itemize}
\underline{Beispiel:}
\begin{figure}[h!]
	\centering\includegraphics[width = \linewidth, scale = 1]{AWpic5.png}
	\caption{}
	\label{beispiel5}
\end{figure}
\paragraph{\underline{Linerarität des Erwartungswerts:}} Für Zufallsvariablen $X_1, \dots, X_n$ und $ X_= a_1X_1 + \dots + a_nX_n + b$ mit $a_1,\dots,a_n, b \in \mathbb{R}$ gilt:
\begin{center}
$\mathbb{E}[X] = a_1\mathbb{E}[X_1] + \dots + a_n\mathbb{E}[X_n] + b$
\end{center}
\paragraph{\underline{Stabiler Menge:}} Knoten, die nicht durch Kanten verbunden sind.
\paragraph{\underline{Satz}} Für jeden Graphen G=(V,E) mit $|V| = n$ und $|E| = m$ bestimmt der Algorithmus (gehe durch die Knotenmenge und entferne den Knoten und inzidente Kanten mit wahrscheinlichkeit 1-p, bei den übrig gebliebene kanten lösche ein Knoten) eine stabile Menge S mit
\begin{center}
$\mathbb{E}[S] \geq np - mp^2$
\end{center}\newpage
\underline{Beweis:}
\begin{figure}[h!]
	\centering\includegraphics[width = \linewidth, scale = 1]{AWpic6.png}
	\caption{}
	\label{beispiel6}
\end{figure}

\paragraph{\underline{Coupon Collector:}\\}
Szenario: Es gibt n verschiedene Bilder in jeder Runde erhalten wir (gleichwahrscheinlich) eines der Bilder\\
X:= Anzahl Runden bis wir alle n Bilder besitzen
Ziel: Berechne $\mathbb{E}[X]$
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{AWpic7.png}
	\caption{}
	\label{ziehungsmoglichkeiten}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{AWpic8.png}
	\caption{}
	\label{ziehungsmoglichkeiten}
	\end{subfigure}
\end{figure}\\
$\Rightarrow$ die Laufzeit der Coupon Collector ist $\mathcal{O}(nlogn + n)$
\paragraph{\underline{Varianz:}} Für ein Zufallsvariable X mit $\mu = \mathbb{E}[X]$ definieren wir die Varianz $Var[X]$ durch:
\begin{center}
$Var[X] := \mathbb{E}[(X -\mu)^2] = \displaystyle\sum_{x \in W_X}(x -\mu)^2 \cdot Pr[X = x]$\\
$Var[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$\\
$Var[a \cdot X + b] = a^2 \cdot Var[X] \qquad X \text{ beliebig, } a,b \in \mathbb{R}$\\
(b verschwindet, da verschieben der werte kein einfluss auf die Abweichung vom Durchschnitt hat)
\end{center}
\paragraph{\underline{Standardabweichung:}}
\begin{center}
$\sigma := \sqrt{Var[X]}$
\end{center}
\paragraph{\underline{Dichten:}} X,Y Zufallsvariablen:\\
\underline{Gemeinsame Dichte:}
\begin{center}
$f_{X,Y}(x,y) := Pr[X=x, Y = y]$
\end{center}
\underline{Randdichte:}
\begin{center}
$f_X(x) = \displaystyle\sum_{y \in W_Y}f:_{X,Y}(x,y)$
\end{center}
\paragraph{\underline{Unabhängigkeit:}} Zufallsvariablen $X_1,\dots,X_n$ heissen unabhängig genau dann, wenn \underline{ für alle} $(x_1,\dots, x_n) \in W_{X_1} \times \dots \times W_{X_n}$ gilt:
\begin{center}
$Pr[X_1 = x_1, \dots, X_n = x_n] = Pr[X_1 = x_1] \cdot \dots \cdot Pr[X_n = x_n]$
\end{center}
Alternativ:
\begin{center}
$f_{X_1,...,X_n}(x_1,...,x_n) = f_{X_1}(x_1) \cdot ... \cdot f_{X_n}(x_n)$ für alle $(x_1,...,x_n) \in W_{X_1} \times ... \times W_{X_n}$
\end{center}
\underline{Beispiel:}
\begin{figure}[h!]
\centering\includegraphics[width = 70mm , scale = 1]{AW9.png}
\caption{}
\label{AW9}
\end{figure}\\
Für \textbf{\underline{zwei}} Indikatorvariablen X und Y gilt:
\begin{center}
X und Y sind unabhängig $\iff f_{X,Y}(1,1) = f_X(1) \cdot f_Y(1)$
\end{center}
\underline{Lemma:} Sind $X_1,...,X_n$ unabhängige Zufallsvariablen und $S_1,...,S_n$ beliebige Mengen mit $S_i \subseteq W_{x_i}$, dann gilt:
\begin{center}
$Pr[X_1 \in S_1,...,X_n \in S_n] = Pr[X_1 \in S_1] \dots Pr[X_n \in S_n]$
\end{center}
 \underline{Korollar:} Sind $X_1,...,X_n$ unabhängige Zufallsvariablen und ist $I =\{ i_1,...,i_k\} \subseteq [n]$, dann sind $X_{i_1},...,X_{i_k}$ ebenfalls unabhängig.\\
\underline{Satz:} $f_1,...,f_n$ seien reellwertige Funktionen $(f_i : \mathbb{R} \rightarrow \mathbb{R}$ für i = 1,...,n). Wenn die Zufallsvariablen $X_1,...,X_n$ unabhängig sind dann gilt dies auch für $f_1(X_1),...,f_n(X_n)$
\paragraph{\underline{Summe von Zufallsvariablen:}} Für zwei unabhängige Zufallsvariablen X und Y sei Z:= X + Y. Es gilt:
\begin{center}
$f_Z(z) = \displaystyle\sum_{x \in W_X}f_X(x) \cdot f_Y(z-x)$
\end{center}
Es folgt:
\begin{center}
$Poisson(\lambda_1) + Poisson(\lambda_2) = Poisson(\lambda_1 + \lambda_2)$
$Bon(n_1,p) + Bin(n_2,p) = Bin(n_1 + n_2, p)$
\end{center}
falls die lambda's und n's unabhängig sind
\paragraph{\underline{Rechenregeln für Momente:}}
\begin{center}
\item $\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y] \qquad \forall X,Y$ 
\item $\mathbb{E}[X \cdot Y] = \mathbb{E}[X] \cdot \mathbb{E}[Y] \qquad \forall$ X,Y unabhängig
\item Var[X + Y] = Var[X] + Var[Y]	 $\qquad \forall$ X,Y unabhängig
\item $Var[X \cdot Y] \neq Var[X] \cdot Var[Y] \qquad$ muss ausgerechnet werden 
\end{center}
\underline{Multiplikativität des Erwartungswerts:} Für unabhängige Zufallsvariablen $X_1,...,X_n$ gilt:
\begin{center}
$\mathbb{E}[X_1 \cdot ... \cdot X_n] = \mathbb{E}[X_1] \cdot ... \cdot \mathbb{E}[X_n]$
\end{center}
\underline{Satz:} Für unabhängige Zufallsvariablen $X_1,...,X_n$ und $X:= X_1 + ... + X_n$ gilt:
\begin{center}
$Var[X] = Var[X_1] + ... + Var[X_n]$
\end{center}
\section{Abschätzen von Wahrscheinlichkeiten}
\paragraph{\underline{Waldsche Identität:}} N und X seien zwei unabhängige Zufallsvariable, wobei für den Wertebereich von N gelte: $W_N \subseteq \mathbb{N}$ Weiter sei:
\begin{center}
$Z:= \displaystyle\sum_{i=1}^{N}X_i$
\end{center}
wobei $X_1,X_2,...$ unabhängige Kopien von X seien. Dann gilt:
\begin{center}
$\mathbb{E}[Z] = \mathbb{E}[N] \cdot \mathbb{E}[X]$
\end{center}
\paragraph{\underline{Ungleichung von Markov:}} Sei X eine Zufallsvariable, die nur nicht negative Werte annimmt. Dann gilt für alle $t \in \mathbb{R}$ mit $t > 0$, dass
\begin{center}
$Pr[X \geq t] \leq \frac{\mathbb{E}[X]}{t} \qquad \forall X \geq 0, \forall t > 0$
\end{center}
\paragraph{\underline{Ungleichung von Chebyshev:}} Sei X eine Zufallsvariable und $t \in \mathbb{R}$ mit $t>0$ Dann gilt:
\begin{center}
$Pr[|X -\mathbb{E}[X]| \geq t] \leq \frac{Var[X]}{t^2} \qquad \forall X, \forall t > 0$\\
\end{center}
insbesondere
\begin{center}
$Pr[X \geq \mathbb{E}[X] + t] \leq \frac{Var[X]}{t^2}$
\end{center}
















\end{document}