\documentclass[8pt]{extreport}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\definecolor{shadecolor}{RGB}{180,180,180}
\title{Parallel Programming Summary}
\author{Gregory Rozanski}
\begin{document}
	\maketitle
	\newpage
\section{Mutual Exclusion}
\subsection{Definitions}
\paragraph{Concurrence:\\}
A form of computing in which several computations are executed during overlapping time periods i.e "concurrently" instead of "sequentially". A concurrent system is one where a computation can advance without waiting for all other computations to complete.
\paragraph{Concurrency Control:\\}
 Concurrency control ensures that correct results for concurrent operations are generated while getting those results as quickly as possible.
\paragraph{Race condition:\\}
A race condition or race hazard is the condition of an electronics,software, or other system where the system's substantive behavior is dependent on the sequence or timing of other uncontrollable events.
\paragraph{Scheduling:\\}
Scheduling is the method by which work is assigned to resources that complete the work. Schedulers are often implemented so they keep all computer resources busy, allow multiple users to share system resources effectively, etc.
\paragraph{Thread of execution:\\}
A thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler.
\paragraph{Critical section:\\}
Concurrent accesses to shared resources can lead to unexpected or erroneous behaviour so parts of the program where the shared resource is  accessed need to be protected in ways that avoid the concurrent access. This protected section is the critical section/region. It cannot be executed by more than one process at a time.
\paragraph{Deadlock:\\}

\textbf{Problem Description:\\}
The problem which mutual exclusion addresses is a problem of resource sharing: how can a software system control multiple processes access to a shared resource, when each process needs exclusive control of that resource while doing its work?\\
\textbf{Solution:\\}
The mutual-exclusion solution to this makes the shared resource available only while the process is in the critical section. It controls access to the shared resource by controlling each mutual execution of that part of its program where the resource would be used.

\paragraph{Language features vs. parallelism: Guidelines}
\begin{itemize}
\item Keep variables as local as possible: global variables can be accessed by various parallel activities
\item If possible, avoid aliasing of references: aliasing can lead to unexpected updates to memory through a process that accesses a seemingly unrelated variable (named differently)
\item If possible avoid mutable state, in particular when aliased: aliasing is no problem if the shared object is immutable
\end{itemize}

\paragraph{Multitasking}
Concurrent execution of multiple tasks/processes. If you only have one CPU its called multiplexing. If we switch fast enough between the processes multiplexing gives us the feeling that the processes are running in parallel. This is advantegous because the
CPU usually waits for inputs and outputs of the memory, hence we can run other processes during this waiting period to increase efficiency.
\paragraph{Process context}
A process is a program executing inside an Operating System. Each running instance of a program is a seperate process.
Each process has a context:
\begin{itemize}
\item Instruction counter: points to next instruction
\item Values in registers, stack and heap
\item Resource handles
\item $\dots$
\end{itemize}
When switching between context we have to temporarily save the context of the current process and load the context of the next process

\paragraph{process lifecycle}
\begin{enumerate}
\item Process created (load from disk)
\item Process in waiting state (Pool of processes which can be executed)
\item Scheduler picks the process and puts it in a Running state
\item Process can enter Blocked state (usually because of I/O, hence it cannot be executed). When block is released it returns to a waiting state
\item Process enters Terminated state, where context is deleted
\end{enumerate}
Each process demands a certain amount of the main memory. In the case where there isnt enough left, swapping takes place in which the context of the current process is put on the Hard Disk creating space for another process. (Slows down the system).
\paragraph{Context Switch} When switching between two processes, the OS interrupts the first one captures its state, loads the state of the second process and executes it. There is alot of overhead generated when switching between processes, hence switching alot between processes is inefficient.
\paragraph{Threads}Threads are:
\begin{itemize}
\item independent sequences of execution
\item running in the same OS process
\end{itemize}
Multiple threads share the same address space hence they execute different code but share the same memory. Threads have the advantage that they are not controlled by protocols and can read and write freely (this also makes it more vulnerable to programming mistakes) . 
\begin{itemize}
\item Threads are not shielded from each other
\item Threads share resources and can communicate more easily
\end{itemize}
Context switching between threads is efficient
\begin{itemize}
\item no change of address space
\item no automatic scheduling
\item no saving/reloading of PCB (OS process) state
\end{itemize}
\paragraph{Multithreading 1 vs. many CPU's} When multiple threads share a single CPU then the threads take turns executing and the others are put in a waiting state. With multiple CPU's e.g 3 threads, 3 CPUs all threads can run constantly increasing performance.


\paragraph{Java Threads} JVM implementation of the thread concepts i.e parallel execution. It is a set of instructions to be executed one at a time, in a specific order. Thread class is part of the core language. Every Java program has at least one execution thread (first one calls main()). A Program ends when all threads finish. Threads can continue to run even if main() returns. Creating a Thread object or calling run() does not start a thread we need to call start().
\paragraph{java.lang.Thread}


\begin{itemize}
\item start() : method called to spawn a new thread (causes JVM to call run() method on object)
\item interrupt() : freeze and throw exception to thread (used to terminate a thread at time of call)
\item sleep(int num) : puts thread to sleep for num ms
\item getID(): gets the thread's ID
\item getName() : gets the name of the currentThread
\item setName() : sets the name of the currentThread
\item currentThread() : returns current Thread
\item setPriority(int num) : Threads can have a priority between 1 and 10. JVM uses the priority of threads to select the one that uses the CPU at each moment. The Scheduler decides wether or not to regard the priority of the threads.
\item getState() : Denotes the status the thread is in
\item join(): thread finishes and returns the result to the sleeping main thread ( May throw InterruptedException)
\item wait() : Consumer goes to sleep i.e status NOT RUNNABLE. If the thread has the lock and is in a state where it cant do anything productive, wait is called such that other threads can access the resource (can only be used if the thread holds the lock). It is recommended to use a while loop around the condition, in order to see that the thread returned from the wait() at a valid time. When not specifying myObject.wait() then wait() = this.wait().
\item notify()/notifyAll() : Changes the state of all threads waiting on the resource to Runnable (can only be used if the thread holds the lock i.e in synchronized block). notify() wakes the highest-priority thread closest to front of object's internal queue. When not specifying myObject.notify() then notify() = this.notify()
\end{itemize}


\section{Creating Java Threads}
\paragraph{OPTION 1: Instantiate a subclass of java.lang.Thread class}
\begin{itemize}
\item Override run method
\item run() is called when execution of that thread begins
\item A thread terminates when run() returns
\item start() method invokes run()
\item calling run() does not create a new thread!
\end{itemize}
\begin{snugshade*}
\begin{tabbing}
class \= ConcurrWriter extends Thread \{ \\
\> public \= void run() \{\\
\> \>  // code here executes concurrently with caller;\\
\> \} \\
\} \\
ConcurrWriter writerThread = new ConcurrWriter();\\
writerThread.start(); // calls ConcurrWriter.run()
\end{tabbing}
\end{snugshade*}

\paragraph{OPTION 2: Use Runnable Interface}
\begin{itemize}
\item single method: public void run()
\item class implements Runnable
\end{itemize}
\begin{snugshade*}
\begin{tabbing}
public \=class ConcurrWriter implements Runnable \{ \\
\> public \= void run() \{\\
\> \>  // code here executes concurrently with caller;\\
\> \} \\
\} \\
ConcurrReader readerThread = new ConcurrReader();\\
Thread t = new Thread(readerThread);\\
t.start(); // calls ConcurrWriter.run()
\end{tabbing}
\end{snugshade*}
Here there it is distinguished between how the programm is executed (the thread) and what is being executed (Runnable)
\paragraph{Busy Waiting:}  By spinning(looping) until each worker's state is TERMINATED. Join (sleep, wakeup) typically incurs context switch overhead. If worker threads are short-lived, busy waiting may perform better.
\paragraph{Exceptions: } Exceptions in a single threaded (sequential) program terminate the program, if not caught. If a worker thread throws an exception, the exception is shown on the console, the behaviour of thread.join() is unaffected, hence the main thread may not be aware of an exception inside a worker thread. Implementing UncaughtExceptionHandler interface allows us to handle unchecked exceptions. Three options:
\begin{itemize}
\item Register exception handler with Thread object
\item Register exception handler with ThreadGroup object
\item Use setDefaultUncaughtExceptionHandler() to register handler for all threads
Handler can then record which threads terminated exceptionally or restart them, or ...
\end{itemize}
\begin{snugshade*}
\begin{tabbing}
public \=class ExceptionHandler implements UncaughtExceptionHandler \{ \\
\> public public Set$<$Thread$>$ threads = new HashSet$<>$();\{\\
\\
\\
@Override\\
public \= void uncaughtException(Thread thread, Throwable throwable)$\{$\\
\>println("An exception has been captured");\\
\> println(thread.getName());\\
\> println(throwable.getMessage());\\
\> $\dots$\\
\> threads.add(thread);\\
\> \} \\
\} \\
\\
\\
public \= class Main $\{$\\
\> public \= static void main(String[] args) $\{$\\
\>$\dots$\\
\> ExceptionHandler handler = new ExceptionHandler();\\
\> thread.setUncaughtExceptionHandler(handler);\\
\>$\dots$\\
\>thread.join();\\
\>if  \=(handler.threads.contains(thread))$\{$ \\
\>\> //bad \\
\> $\}$ \= else  $\{$ \\
\>\> // good \\
\> $\}$\\
 $\quad \}$\\
$\}$\\

\end{tabbing}
\end{snugshade*}
\paragraph{Thread Safety:} This implies program safety and refers to "nothing bad ever happens", in any possible interleaving.
\paragraph{Liveness:} "eventually something good happens" (e.g endless loops are an example of liveness hazards in sequential programming). Threads makes liveness hazards more frequent: If ThreadA holds a resource( e.g a file handle) exclusively, then ThreadB might be waiting for that resource forever. Hence liveness means that progress will be made.
\paragraph{Examples of safety properties:}
\begin{itemize}
\item absence of data races
\item mutual exclusion
\item linearizability
\item atomicity
\item schedule-deterministic
\item absence of deadlock
\item custom invariants
\end{itemize}
\paragraph{Synchronized} Multiple threads may read/write the same data (shared objects,global data). To avoid bad interleaving we use explicit synchronization. In Java, all objects have an internal lock, called intrinsic/monitor lock. Synchronized operations lock the object, hence no other thread can successfully lock and use the object and must wait until the lock is freed. Generally if accessing shared memory, make sure itis done under a lock, if not the code is prone to a data race.
\paragraph{Synchronized Methods: } A synchronized method grabs the object or class's lock at the start , runs to completion, then releases the lock. This is useful for methods whose entire bodies are critical sections, and thus should not be entered by multiple threads at the same time. A synchronized method is a critical section with guaranteed mutual exclusion
\begin{snugshade*}
\begin{tabbing}
// synchronized method: locks on "this" object \\
public synchronized type name(parameters) $\{ \dots \}$\\
\\
// synchronized static method: locks on the given class\\
public static synchronized type name(parameters) $\{ \dots \}$\\
\\
\\
Synchronized Blocks:\\
\\
sync\=hronized (object) $\{$\\
\> statement(s); //critical sections\\
$\}$\\
\end{tabbing}
\end{snugshade*}
\paragraph{Synchronized Blocks: } Enforces mutual exclusion with regards to some object. Every Java object can act as a lock for concurrency: A thread $T_{1}$ can ask to run a block of code, synchronized on a given object O. The synchronized block makes sure there is no interleavings of the statements inside the block, it does not prevent other threads from executing statements outside of the block, hence it is still possible for bad interleavings to happen with statements outside the block
\begin{itemize}
\item If no other thread has locked O, then $T_{1}$ locks the object and proceeds
\item If another thread $T_{2}$ has already locked O, then $T_{1}$ becomes blocked and must wait until $T_{2}$ is finished with O (that is, unlocks O). Then, $T_{1}$ is woken up, and can proceed 
\end{itemize}
\paragraph{Reentrant: } Locks are recursive. A thread can request to lock an object it has already locked, and will lock it, the thread will then release the lock multiple times.
\paragraph{Synchronization granularity: } Using multiple locks to allow multiple threads to work on code while still being protected.
\paragraph{Synchronized and Exception} If an exception is triggered in the middle of a synchronized bock, then the lock released, as if the synchronized scope ends right at the point where the exception is thrown. When the exception is caught, then the exception handler is executed. If there is no exception handler, then the exception is propagated back down to the caller of the method. Any side effects are not reverted, they do take effect even if exceptions are thrown.

\paragraph{Producer-Consumer: } The Producer puts items into a shared buffer (shared resource), the consumer takes them out, consumption is only possible if buffer isn't empty.
\paragraph{Pseudo-Code Implementation of synchronized block}:
\begin{figure}[h!]
  \begin{subfigure}[b]{0.4\linewidth}
  \includegraphics[width=\linewidth]{synchronizedPseudo1.png}

  \label{fig: synchronized block}
  \end{subfigure}
\begin{subfigure}[b]{0.4\linewidth}
  \includegraphics[width=\linewidth]{synchronizedPseudo2.png}
 
  \label{fig: wait/notify}
\end{subfigure}
  \caption{Pseudo implementation of synchronized block}
\end{figure}

\section{Parallel Architectures}
\paragraph{Parallelism:} Use extra resources to solve a problem faster
\paragraph{Concurrency:} Correctly and efficiently manage access to shared resources. 
\paragraph{Distributed computing:} Physical separation, administrative separation, different domains, multiple systems
\paragraph{Von Neumann architecture:} Program data and program instructions share the same memory.

\paragraph{CPUs and Memory Hierarchies}Caches are preloaded data readily available to speed up access time. 
\begin{itemize}
\item Goal: Allow cores to work int parallel, on their own, fast memory
\item CPU reads/writes values from/to main memory, to compute with them, with a hierarchy of memory caches in between. Faster memory is more expensive, hence smaller: L1 is 5x faster than L2, which is 30x faster than main memory, which is 350x faster than disk.
\item Synchronisation between caches is taken care of by cache coherence protocols(e.g MESI see notes page 3)
\item Concurrency Hazard: cores may pre-/postpone reads/writs from/to cache; memory barriers needed to prevent problems with parallel code. (In Java memory barriers are automatically insterted if e.g synchronized is used.)
\end{itemize}

\paragraph{Vectorization: } 
\begin{itemize}
\item Goal: improve performance by using specialized vector instructions
\item SIMD: Single Instruction, applied to Multiple Data
\item Requires vectorised code: code that uses the vector instructions provided by the target platform (CPU)
\item Compilers(C++, JVMs JIT,...) attempt to detect vectorization opportunities $\rightarrow$ fully automated, but little or no control over if/where/how
\item platform specific libraries (intrinsics,C/C++) expose vector instructions to developers $\rightarrow$ manual effort, but full control
\item Poses no (additional) safety risk to concurrency
\end{itemize}
\paragraph{Instruction Stream :} Instructions given to the CPU to execute
\paragraph{Instruction Level Parallelism (ILP) } 
\begin{itemize}
\item Goal: improve CPU performance by internal parallelisation
\item CPU/Core detects independent operations in its instruction stream
\item These may be executed in parallel inside the CPU if enough functional units (e.g floating-point unit,...) are available
\item Various measures to increase potential for instruction parallelization. E.g speculatively execute instructions in parallel, even if result may not be used
\item Concurrency hazard: cores only locally consider dependencies in their instruction stream, not globally across all cores. (Java e.g synchronized automatically adds memory barriers to prevent problematic reordering)
\item Compilers may also reorder instructions; similar problems, same solution
\end{itemize}
\subsection{Pipelining}
\paragraph{Balanced Pipeline} All steps require the same time
\paragraph{Throughput} The amount of work that can be done by a system in a given period of time (How much can go through the pipeline in a given time)
\begin{itemize}
\item In CPUs : $\#$ of instructions completed per second
\item The larger the throughput the better
\end{itemize}
\paragraph{Throughput bound} = $\frac{1}{max(computationtime(stages))}$\\
1:= unit of work (e.g one instruction, one network package, ...)\\
max(computationtime(stages)) := the time of the longest step in the pipeline\\
The bound gives the throughput when the pipeline is at full utilization i.e it ignores lead-in and lead-out time 
\paragraph{Latency} Time needed to perform a given computation (I.e how long does it take one item to go through the pipeline)
 \begin{itemize}
\item In CPU: time required to execute a single instruction in the pipeline
\item Lower is better
\item Pipeline latency is only constant over time if  the pipeline is balanced (i.e each step takes the same time)
\item more input means our bound is less exact
\end{itemize}
\paragraph{Latency bound} = $\#stages \cdot max(computationtime(stages))$
\paragraph{Optimizing an unbalanced pipeline} (E.g Clothes Washing) w: 5s d:10s f: 5: c:10, the given pipeline is unbalanced because drying and putting clothes in the closet takes more than the washing and folding. An attempt to balance the pipeline to get a constant latency would be to artificially increase the length of all steps to 10s, but in this case we would decrease the throughput. The other option is to add additional functional units i.e another dryer and closet increasing the total number of steps from 4 to 6 w:5s d1:4s d2:6 f:5 c1:4 c2:6, we then increase the duration of all steps to the duration of the longest step i.e 6, hence the pipeline is balanced and the throuput increased.
\paragraph{Throughput vs Latency} Pipelining typically adds constant time overhead between individual stages (synchronization,communication), hence infinitely small pipeline steps are not practical and time it takes to get one complete task through the pipeline may take longer than with a serial implementation.
\section{Basic Concepts in Parallelism}
\paragraph{Expressing Parallelism} The goal is to split up work of a single program into parallel tasks. This can be done Explicitly/Manually(task/thread parallelism) or Implicitly i.e Done automatically by the system (user expresses an operation and the system does the rest.
\paragraph{Work Partitioning $\&$ Scheduling}
\begin{itemize}
\item work partitioning (task/thread decomposition)
\begin{itemize}
\item split up work into parallel tasks/threads
\item done by user
\item A task is a unit of work
\item number of parititions should be larger than the number of processors
\end{itemize}
\item scheduling
\begin{itemize}
\item assign tasks to processors
\item typically done by the system
\item goal is full utilization i.e no processor is ever idle
\end{itemize}
\end{itemize}
\paragraph{Coarse vs Fine granularity}
\begin{itemize}
\item Fine granularity
\begin{itemize}
\item more portable (can be executed in machines with more processors
\item better for scheduling
\item but: if scheduling overhead is comparable to a single task $\rightarrow$ overhead dominates
\end{itemize}
\item Task granularity guidelines
\begin{itemize}
\item As small as possible but, significantly bigger than scheduling overhead
\end{itemize}
\end{itemize}
\paragraph{Scalability} An overloaded concept: e.g how well a system reacts to increased load, for example clients in a server. In parallel programming:
\begin{itemize}
\item speedup when we increase processors
\item what happens if $\# processors \rightarrow \quad \infty$
\item program scales linearly $\rightarrow$ linear speedup
\end{itemize}
\paragraph{Parallel Performance} Sequential execution time: $T_{1}$\\
Execution time $T_{p}$ on p CPUs
\begin{itemize}
\item $T_{p} = T_{1}/p$  (Perfect Case)
\item $T_{p} > T_{1}/p$  (Performance loss,what normally happens)
\item $T_{p} < T_{1}/p$  (Can happen but unusual)
\end{itemize}
\paragraph{Parallel Speedup}
Speedup $S_{p}$ on p CPUs $S_{p} = T_{1}/T_{p}$ :
\begin{itemize}
\item $S_{p} = p$ linear speedup (Perfect Case)
\item $S_{p} < p$ sub-linear speedup (Performance loss,what normally happens)
\item $S_{p} > p$ super-linear speedup (Can happen but unusual)
\end{itemize}
Speedup is not only dependant on the program but also on the input.\\
Why $S_{p} < p$?\\
Programs may not contain enough parallelism (some parts might be sequential)\\
Overheads introduced by parallelization (typically associated with synchronization)\\
Architectural limitations (e.g memory contention)\\


\paragraph{Efficiency} $S_{p}/p$ how efficient is a multicore system for a given task
\paragraph{Amdahl's Law} Execution time $T_{1}$ of a program falls into two categories:
\begin{itemize}
\item Time spent doing non-parallelizable serial work
\item Time spent doing parallelizable work
\end{itemize}
Denoted: $W_{ser}, W_{par}$\\
Given P workers available to do parallelizable work, the times for sequential execution and parallel execution are: \\
$T_{1} = W_{ser} + W_{par}$\\
Resulting in a bound on speed up: $T_{p} \geq W_{ser} + \frac{W_{par}}{P}$\\
$\Rightarrow$ Amdahls Law:	$S_{p} \leq \frac{W_{ser}+W_{par}}{W_{ser} + \frac{W_{par}}{P}}$\\
We define $\mathit{f}$ as the non-parallelizable serial fractions of the total work. The following equalities hold:
\begin{itemize}
\item $W_{ser} = \mathit{f}T_{1}$
\item $W_{par} = (1-\mathit{f})T_{1}$
\end{itemize}
$\Rightarrow S_{p} \leq \frac{1}{\mathit{f} + \frac{1-\mathit{f}}{P}} \quad \Rightarrow S_{\infty} \leq \frac{1}{\mathit{f}}$
\paragraph{Gustafson's Law}
Observations:
\begin{itemize}
\item consider problem size
\item run-time, not problem size, is constant
\item more processors allows to solve larger problems in the same time
\item parallel part of a program scales with the problem size
\end{itemize}
$\mathit{f}:$ sequential part, $T_{wall} = available time$ \\
$W = p(1- \mathit{f})T_{wall} + \mathit{f}T_{wall}$\\
$S_{p} = \frac{S_{p}}{S_{1}} = \mathit{f} + p(1-\mathit{f}) = p - \mathit{f}(p-1)$ 
\section{Divide and Conquer}
\paragraph{fork/join} Style of programming using start,run,join methods. They create a "happens before before relation", the ordering of the memory access is important and must be considered.
\paragraph{Approach to Divide and Conquer} In theory you can divide down to single elements, do all your resultcombining in parallel and get optimal speedup. In practice, creating all those threads and communicating swamps the savings hence:
\begin{itemize}
\item Use a sequential cutoff, typically around 500-1000 (eliminates almost all the recursive thread creation(bottom levels of tree)
\item Do not create two recursive threads, create one and do the other "yourself"
\item If given enough processors, total time is height of the tree $\mathcal{O}(logn)$ 
\item Often relies on operations being associative
\end{itemize}
\paragraph{\underline{Executor Service:}} Manages asynchronous tasks. ExecutorService is a Java Class which takes in a users submitted task and returns a "Future" object. Two ways to submit a task to the ExecutorService:
\begin{itemize}
\item .submit(Callable $<T>$ task) $\rightarrow$ Future$<T>$  (Returns result)
\item .submit(Runnable task) $\rightarrow$ Future$<?>$ 	(Does not return result)
\end{itemize} 
\underline{Beispiel:}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg1.png}
	\caption{creating task}
	\label{ziehungsmoglichkeiten}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg2.png}
	\caption{creating executer,submitting}
	\label{ziehungsmoglichkeiten}
	\end{subfigure}
\end{figure}
\newline
\underline{Recursive Sum with ExecutorService:}
\begin{figure}[h!]
	\centering\includegraphics[width = 50mm,scale = 1]{PProg3.png}
	\caption{}
	\label{PProg3}
\end{figure}
\newline
The get method blocks until the method which the Future object refers to is finished. The above implementation does not work because the ExecutorService is bound to a certain number of threads, hence we will eventually run out of threads and the tasks will end up waiting. The ExecutorService is not meant to be used when you need to wait for results of other tasks (divide and conquer). A possible approach is to decouple work partitioning from solving the problem. We split the array into chunks and create a task per chunk, we submit these into the ExecutorService and combine the results. When one task is finished the thread is freed and assigned to another task. I.e flat patterns (threads arent waiting) are good for the ExecutorService.
\paragraph{\underline{Cilk-style:}} Tasks:
\begin{itemize}
\item execute code
\item spawn other tasks
\item wait for results from other tasks
\end{itemize}
A graph is formes based on spawning tasks. There is an edge from node u to node v if task v was created by task u. Source vertice must finish first before destination starts.
With Cilk there is no waiting for a certain task, but instead we wait for all tasks created until now to complete. There are no deadlocks in Cilk style programming (The task graphs are directed acyclic graphs).
\begin{figure}[h!]
	\centering\includegraphics[width = 80mm,scale = 1]{PProg4.png}
	\caption{}
	\label{PProg4}
\end{figure}
\newline
\underline{Task Parallelism:}
\begin{itemize}
\item Tasks can execute in parallel, but they dont have to. The assignment of tasks to CPUs/Cores is up to the scheduler
\item The task graph is dynamic and unfolds as execution proceeds(input dependent). A wide task graph means more parallelism
\end{itemize}
\underline{Performance Model:} Tasks become available as computation progresses. We can execute the graph on p processors, the scheduler assigns tasks to the processors, hence the execution time $T_p$ can vary depending on the scheduler being used.
\begin{itemize}
\item $T_p$ execution time on p processors
\item $T_1$ work(total amount of work) i,e the sum of the time cost of all nodes in graph (as if we executed graph sequentially)
\item $\frac{T_1}{T_p} \rightarrow$ speedup
\item $T_\infty$ span, critical path,computational depth: Time it takes on infinite processors i.e the longest path from root to sink
\item $\frac{T_1}{T_\infty} \rightarrow$ parallelism i.e maximum possible speedup
\item Lower bounds:
\begin{itemize}
\item $T_p \geq \frac{T_1}{p}$
\item $T_p \geq T_\infty$
\end{itemize}
\item $T_p \approx \frac{T_1}{p} + T_\infty$
\end{itemize}
\underline{Scheduler is an algorithm for assigning tasks} $T_p$ depends on the the scheduler. $\frac{T_1}{P}$ and $T_\infty$ are fixed
\begin{figure}[h!]
	\centering\includegraphics[width = 80mm,scale = 1]{PProg5.png}
	\caption{}
	\label{PProg5}
\end{figure}
The above figure shows that different schedulers can have different $T_p$. The boxes represent what is scheduled in each step.
\section{ForkJoin Framework $\&$ Task Parallel Algorithms}

\paragraph{\underline{ForkJoin Framework:}} Designed to meet the needs of divide-and-conquer fork-join parallism.

\begin{figure}[h!]
	\centering\includegraphics[width = 80mm,scale = 1]{PProg6.png}
	\caption{}
	\label{PProg6}
\end{figure}
\begin{itemize}
\item .fork() $\rightarrow$ create a new task (Computation Graph: Ends a node and makes two outgoing edges i.e new thread and continuation of current thread)
\item .join() $\rightarrow$ return result when task is done (Computation Graph: Ends a node and makes a node with two incoming edges i.e task just ended last node of thread joined on)
\item .invoke() $\rightarrow$ submits task and waits until it is completed
\item .submit() $\rightarrow$ submits task (recieves a Future)
\end{itemize} 
\newpage
\underline{Recursive sum with ForkJoin:}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg7.png}
	\caption{}
	\label{PProg7}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg8.png}
	\caption{}
	\label{PProg8}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg9.png}
	\caption{}
	\label{PProg9}
	\end{subfigure}
\end{figure}
The ForkJoinPool creates a number of threads equal to the number of available processors. The code aboce performs poorly in java. The following fix is possible:
\begin{figure}[h!]
	\centering\includegraphics[width = 80mm,scale = 1]{PProg10.png}
	\caption{}
	\label{PProg10}
\end{figure}
\paragraph{\underline{Reductions:}} Produce a single answer from collection via an associative operator (e.g max, count, leftmost,rightmost,...) (non examples: median, subtraction, exponentiation). (Recursive) results dont have to be a single number or strings. They can be arrays or objects with multiple fields. (e.g Histogram of test results is a variant of sum).
But some things are inherently sequential i.e how we process arr[i] may depend entirely on the result of processing arr[i-1].
\paragraph{\underline{Maps:}} A map operates on each element of a collection independently to create a new collection of the same size, hence there is no combining results.
\paragraph{\underline{When to use Maps or Reduction:}}
\begin{itemize}
\item Data structure matters!
\item Parallelism is still beneficial for expensive per-element operations on a sequential Datastructure (e.g Linked Lists)
\item For parallelism, balanced trees are generally better than lists so that we can get to all the data exponentially faster $\mathcal{O}(logn)$ vs $\mathcal{O}(n)$
\end{itemize}

\paragraph{\underline{The prefix-sum problem:}} Example used to show that inherently sequential programs can in fact be made parallel. 
Problem Statement:
\begin{itemize}
\item Given int[] input
\item Produce int[] output
\item output[i] = input[0] + input[1] + ... + input[i]
\end{itemize}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg11.png}
	\caption{}
	\label{PProg11}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg12.png}
	\caption{}
	\label{PProg12}
	\end{subfigure}
	
\end{figure}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg13.png}
	\caption{}
	\label{PProg13}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg1622.png}
	\caption{}
	\label{PProg1622}
	\end{subfigure}
\end{figure}
We get a parallel speedup at the expense of using more memory.

\paragraph{\underline{Pack Problem:}} Given an array input, produce an array output containing only elements such that f(elt) is true (i.e elements such that some property holds e.g elt > 10). How is this Parallelizable? The work is $\mathcal{O}(n)$. Difficulty arises when trying to find the position of the current element in the result as its position depends on how many elements before it satisfy the condition. Solution (Using condition elt > 10):
\begin{figure}[h!]
	\centering
	\includegraphics[width = 60mm,scale = 1]{PProg17.png}
	\caption{}
	\label{PProg17}
\end{figure}
\section{Shared memory concurrency, locks and data races}
\paragraph{Managing State}
\begin{itemize}
\item \underline{Immutability} Data does not change. This is the best option and should be used when possible
\item \underline{Isolated Mutability} Data can change, but only one thread/task can access them
\item \underline{Mutable/Shared data} Data can change, multiple Tasks/Threads can potentially access the data
\end{itemize}
\paragraph{\underline{Mutable/Shared data:}} This is present in shared memory architectures. Concurrent accesses may lead to inconsistencies, hence we must protect the state by allowing only one thread/task access the memory at a time. We can achieve this by using the following methods:
\begin{itemize}
\item \underline{Locks:} Mechanism to ensure exclusive access/atomicity (Assume that there will be other threads that will try to modify the memory)
\item \underline{Transactional memory:} Programmer describes a set of actions that need to be atomic (Perform actions and only after completion do we check if there was a conflict, if a conflict occured we rollback)
\end{itemize}
\paragraph{\underline{Mutual Exclusion:}} When one thread uses a resource another thread must wait until its free. (The resource is known as a critical section). Implementing critical sections is done by the programmer as the compiler is not capable of recognizing them and bad interleavings can occur.
\paragraph{\underline{Lock Object in Java:}} Locks ensure that given simultaneous acquires and/or releases, a correct thing will happen.
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg14.png}
	\caption{}
	\label{PProg14}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
	\includegraphics[width = \linewidth, scale = 1]{PProg15.png}
	\caption{}
	\label{PProg15}
	\end{subfigure}
\end{figure}




\end{document}